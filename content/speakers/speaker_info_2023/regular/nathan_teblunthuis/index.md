---
title: "Nathan TeBlunthuis"
url: "2023/regular/nathan_teblunthuis"
---

### Misclassification Causes Bias in Regression Models: How to Fix It Using the MisclassificationModels Package

Automated classifiers (ACs), often built via supervised machine learning, can categorize large and statistically powerful samples of data
ranging from text to images and video, and have become widely popular measurement devices in many scientific and industrial fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses—unless such analyses account for these errors. 

In principle, existing statistical methods can use “gold standard”
validation data, such as that created by human annotators and often used to validate predictiveness, to correct misclassification bias and produce consistent estimates. I will present an evaluation of such methods, including a new method implemented in the experimental R package misclassificationmodels, via Monte-Carlo simulations designed to reveal each method’s limitations. The results show the new method is both versatile and efficient. 

In sum, automated classifiers, even those below common accuracy standards or making systematic misclassifications, can be useful for measurement with careful study design and appropriate error correction methods.

<br><br>

<table>
  <tr><td><img width="300px" style="float: left; padding: 0px 20px 0px 0px;" 
           src="../../../../img/logo/logo_2023/logo_2023.png" alt="SPEAKER NAME HERE"></td>
  <td>
      <h5>Pronouns: </h5>
      <h5>Seattle, WA, USA</h5>
      Bio coming soon
      </td></tr>

</table>


